"""\nMODULE 2 â€” Scraper Engine (Playwright Pagination/Scroll Controller)\nUsage:\n    from core.scraper_engine import ScraperEngine\n    engine = ScraperEngine(agent_config_id=1)\n    pages_html = await engine.run()\n"""\nimport sys\nfrom pathlib import Path\nsys.path.insert(0, str(Path(__file__).resolve().parent.parent))\n\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom core.models import SessionLocal, AgentConfig\nfrom core.database import get_session_for_agent\n\nlogger = logging.getLogger("scraper_engine")\n\n\nclass ScraperEngine:\n    def __init__(self, agent_config_id: int):\n        self.agent_config_id = agent_config_id\n        self.config = None\n        self._load_config()\n\n    def _load_config(self):\n        db = SessionLocal()\n        try:\n            agent = db.query(AgentConfig).filter(AgentConfig.id == self.agent_config_id).first()\n            if not agent:\n                raise ValueError(f"AgentConfig ID {self.agent_config_id} not found.")\n            self.config = {\n                "id": agent.id, "nom": agent.nom, "target_url": agent.target_url,\n                "target_api": agent.target_api or "playwright",\n                "template_type": agent.template_type or "catalogue",\n                "requires_scroll": agent.requires_scroll or False,\n                "pagination_selector": agent.pagination_selector,\n                "max_pages": agent.max_pages or 1, "tenant_id": agent.tenant_id,\n            }\n        finally:\n            db.close()\n\n    async def _scroll_to_bottom(self, page, max_scrolls=20, pause=1.5):\n        previous_height = 0\n        for i in range(max_scrolls):\n            current_height = await page.evaluate("document.body.scrollHeight")\n            if current_height == previous_height:\n                logger.info(f"  Scroll stable after {i} scrolls.")\n                break\n            previous_height = current_height\n            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")\n            await asyncio.sleep(pause)\n\n    async def _extract_page_html(self, page) -> str:\n        return await page.content()\n\n    async def _click_next_page(self, page, selector: str) -> bool:\n        try:\n            next_btn = await page.query_selector(selector)\n            if next_btn:\n                is_disabled = await next_btn.get_attribute("disabled")\n                aria_disabled = await next_btn.get_attribute("aria-disabled")\n                if is_disabled or aria_disabled == "true":\n                    return False\n                await next_btn.click()\n                await page.wait_for_load_state("networkidle", timeout=10000)\n                await asyncio.sleep(1)\n                return True\n            return False\n        except Exception as e:\n            logger.warning(f"  Pagination click failed: {e}")\n            return False\n\n    async def run(self) -> list:\n        try:\n            from playwright.async_api import async_playwright\n        except ImportError:\n            logger.error("Playwright not installed.")\n            return []\n        cfg = self.config\n        pages_html = []\n        self._update_agent_status("RUNNING")\n        start_time = datetime.utcnow()\n        try:\n            async with async_playwright() as p:\n                browser = await p.chromium.launch(headless=True)\n                context = await browser.new_context(\n                    viewport={"width": 1920, "height": 1080},\n                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"\n                )\n                page = await context.new_page()\n                await page.goto(cfg["target_url"], wait_until="domcontentloaded", timeout=30000)\n                await page.wait_for_load_state("networkidle", timeout=15000)\n                for page_num in range(1, cfg["max_pages"] + 1):\n                    if cfg["requires_scroll"]:\n                        await self._scroll_to_bottom(page)\n                    html = await self._extract_page_html(page)\n                    pages_html.append(html)\n                    if page_num < cfg["max_pages"] and cfg["pagination_selector"]:\n                        if not await self._click_next_page(page, cfg["pagination_selector"]):\n                            break\n                    elif page_num < cfg["max_pages"]:\n                        break\n                await browser.close()\n            duration = (datetime.utcnow() - start_time).total_seconds()\n            self._update_agent_status("IDLE", duration=duration)\n        except Exception as e:\n            logger.error(f"Scraping failed: {e}")\n            self._update_agent_status("ERROR", error_msg=str(e)[:500])\n        return pages_html\n\n    def _update_agent_status(self, status, duration=None, error_msg=None):\n        db = SessionLocal()\n        try:\n            agent = db.query(AgentConfig).filter(AgentConfig.id == self.agent_config_id).first()\n            if agent:\n                agent.status = status\n                agent.last_run = datetime.utcnow()\n                if duration is not None:\n                    agent.last_run_duration_s = duration\n                if error_msg:\n                    agent.error_message = error_msg\n                elif status != "ERROR":\n                    agent.error_message = None\n                db.commit()\n        except Exception as e:\n            db.rollback()\n        finally:\n            db.close()\n\n    def get_target_session(self):\n        return get_session_for_agent(self.agent_config_id)\n